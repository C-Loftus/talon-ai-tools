---
title: Customizing behavior
description: Customizing the model behavior
---

talon-ai-tools can be configured by changing settings in any `.talon` file. You can copy any of the following settings, uncomment them, and change the values to customize which model you use or its runtime behavior.

import LocalFile from "../../../components/LocalFile.astro";

<LocalFile data="../../../talon-ai-settings.talon.example" />

## Adding custom prompts

You do not need to fork the repository to add your own custom prompts. Copy the file below, place it anywhere inside your talon `user/` directory and follow the pattern of the key value mapping.

<LocalFile data="../../../GPT/lists/customPrompt.talon-list.example" />

## Advanced Customization

### Configuring Model Name

The word `model` is the default prefix before all LLM commands to prevent collisions with other Talon commands. However, you can change or override it. To do so just create another talon list with the same name and a higher specificity. Here is an example that you can copy and past into your own configuration files

```yml title="myCustomModelName.talon-list"
list: user.model
-

# Whatever you say that matches the value on the left will be mapped to the word `model`
# and thus allow any model commands to work as normal, but with a new prefix keyword

my custom model name: model
```

### Providing Custom User Context

In case you want to provide additional context to the LLM, there is a hook that you can override in your own python code and anything that is returned will be sent with every request. Here is an example:

```py
from talon import Context, Module, actions

mod = Module()

ctx = Context()


@ctx.action_class("user")
class UserActions:
    def gpt_additional_user_context():
        """This is an override function that can be used to add additional context to the prompt"""
        result = actions.user.talon_get_active_context()
        return [
            f"The following describes the currently focused application:\n\n{result}"
        ]
```
